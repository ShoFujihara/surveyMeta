% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/evaluate.R
\name{evaluate_extraction}
\alias{evaluate_extraction}
\title{Evaluate extraction quality against official metadata}
\usage{
evaluate_extraction(extracted, official = NULL, survey_id = NULL)
}
\arguments{
\item{extracted}{Named list from \code{\link{extract_metadata}}.}

\item{official}{Named list from \code{\link{fetch_official_metadata}}.
If NULL and \code{survey_id} is provided, fetched automatically.}

\item{survey_id}{Character. SSJDA survey number. Used to fetch official
metadata if \code{official} is NULL.}
}
\value{
An survey_evaluation object (data.frame) with columns: field,
field_type, official, extracted, similarity, precision, recall, f1,
match_type.
}
\description{
Compares extracted metadata (from \code{\link{extract_metadata}}) with
official metadata (from \code{\link{fetch_official_metadata}}) and
returns field-by-field similarity scores using appropriate metrics
for each field type.
}
\details{
Field types and their metrics:
\itemize{
\item \strong{Exact match fields} (data_type, unit_of_analysis): Accuracy (0 or 1)
\item \strong{Set fields} (cessda_topic, sampling_procedure, mode_of_collection):
Precision, Recall, F1
\item \strong{Text fields} (survey_overview, survey_target, survey_conductor, etc.):
Jaccard similarity on character bigrams
\item \strong{Structured fields} (sample_size): Custom comparison
}
}
\examples{
\dontrun{
result <- extract_metadata("survey.pdf")
eval_result <- evaluate_extraction(result, survey_id = "0987")
print(eval_result)
}

}
